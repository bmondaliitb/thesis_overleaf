\chapter{Analysis overview}
\label{analysis_overview}
This chapter presents an overview of the analysis performed to measure the differential cross-sections of the \tty process. Theoretical studies have shown that in the narrow-width approximation, the cross-section of the \tty process can be factorized into two main contributions: the first describes photon emission during the production phase, while the second corresponds to emission from the top quark decay products, with the former being sensitive to top-photon coupling. These scenarios are categorized as $\textbf{\tty production}$ (where the photon is radiated in the production part, i.e. from an initial-state parton or from an off-shell top quark) and $\textbf{inclusive \tty}$ (regardless of the origin of the photon). The cross-section for both processes is measured using the same strategy. Measurement is performed in single-lepton and dilepton final states of the \ttbar decay, in a single-lepton channel one of the W bosons decays leptonically whereas in the dilepton channel both the W bosons decay leptonically. 

The cross-section is measured from the data set containing proton-proton collision at a center-of-mass energy of 13 TeV collected by the ATLAS detector during its Run 2 phase (2015-2018). The dataset corresponds to a total luminosity of $140 \ fb^{-1}$. Cross-section is measured within a fiducial phase space which is defined at particle level [\cref{sec:fiducial-phase-space}]. The particle level means that the final state particles are used to define the phase space. The fiducial phase space is defined in such a way that the kinematic selections are close to the detector level (reconstruction level) phase space. The differential cross-sections are measured as a function of photon kinematic variables, angular variables related to the photon and leptons or the jets, and, in the dilepton channel, the angular separation between two leptons in the event (details of the observables mentioned in \cref{tab:listvariables}). The measurements are performed separately for the single lepton channel, dilepton channel, and combined channel (where applicable).

Events are selected in the single-lepton channel with one lepton, one photon, and four or more jets among which at least one jet is originating from the hadronisation of a b-quark (b-tagged). In the dilepton channel events are selected with two opposite-charged leptons, one photon and two or more jets with at least one b-tagged jet (details of the selection mentioned in \cref{sec:event-selection}). MC simulation is used to model signals and different background processes. MC-simulated events are interfaced with the ATLAS detector simulation algorithm. Physics objects (lepton, photon, jet, b-tagged jet, MET) are reconstructed from digitized detector signatures using the same object reconstruction algorithm used in data. At the particle level, MC simulated events are properly reweighted to take into account pileup effects and scaled to match the luminosity of the data. At the reconstruction level object selection efficiencies are taken into account in addition to the pileup effects and also events are scaled to match the luminosity of the data. Data and MC events are compared to understand the composition of processes predicted by the Standard Model (SM) in different phase space volumes. Details on the MC simulation are mentioned in \cref{sec:data-and-mc-simulations}. Details on the object reconstruction are mentioned in \cref{sec:trigger-object-event-selection}.


The events in the analysis are categorized based on the origin of the photon. Four main categories are considered: \tty production, where the photon is radiated during the production phase from an initial-state parton or from an off-shell top quark, and \tty decay, which includes events where the photon originates from the top quark decay products, there are events with fake photons (\efake and \hfake events, mentioned in the next paragraphs), also there are events with prompt photon but the origin of the photon is not top quark (these are denoted as prompt photon events). The details of the categorization are mentioned in \cref{sec:photon-categorisation}.

One of the important background contributions comes from the misidentification of an electron as a photon. These events are denoted as \efake throughout the thesis. It is an important background source in the single-lepton channel. Mainly the \ttbar events with dileptonic decay ($ee$ or $e\mu$ channel) or \zee events where one electron is misidentified as a photon contribute to this background. A slight mismodelling is observed between data and MC for this kind of event. Using data-driven tag and probe approach using \zee process scale factors are obtained in bins of photon $p_T$, $|\eta|$ and conversion type. The estimation of the scale factor is detailed in \cref{sec:background-estimation-efake}. The MC events with \efake are corrected with the scale factor. Uncertainty on the scale factor is correctly propagated in the analysis.

Another important background contribution comes from the events with photon originating from hadronic decays. Also, there are events in which a jet/hadron is misidentified as a photon. These two contributions are referred to as hadronic fake events (\hfake). A slight mismodelling is observed between data and MC for the \hfake events. Using the data-driven ABCD method scale factors are obtained in bins of photon $p_T$, $|\eta|$ and conversion type (detailed in ~\cite{DiezPardos:2781712}]. The MC events with \hfake are corrected with the scale factor. Uncertainty on the scale factor is correctly propagated in the analysis.

Another background contribution comes from the so-called fake leptons. These are events with non-prompt leptons misidentified as prompt leptons or jets that are misidentified as leptons. The non-prompt leptons could be coming from the decay of a heavy hadron (bottom or charm hadrons), or from a photon conversion or they can be produced from the decay of a pion or a kaon. The contribution of fake lepton events is estimated from data using the matrix method, mentioned in \cref{sec:background-estimation-lfake}. Estimated fake lepton events are used along with the signal and background simulated events. Proper uncertainties are assigned to the fake lepton events.

A multi-class neural network (NN) is employed in data and in MC-simulated events to create signal-enriched (SR) and background-enriched control regions (CR). In the single-lepton channel, a 4-class neural network is trained with the following classes: \tty production, \tty decay, fake photon processes (\efake and \hfake) and other prompt photon backgrounds. In the dilepton channel, where background contribution from other sources than \tty decay is much smaller a binary classification neural network is employed. This network is designed to separate \tty production signal from all other background sources. The output of the neural network is used to define signal regions (SR) and control regions (CRs). In the single-lepton channel, the SR is defined to be a signal-enriched region where the \tty production signal is dominant over the \tty decay and other background processes. The CRs are defined to be background-enriched regions where the \tty decay and other background processes are dominant over the \tty production signal. The SR and CRs are defined in such a way that they are orthogonal to each other. In the dilepton channel, a cut at 0.6 on the NN discriminant is used to define the SR and CRs. The neural network implementation and the definition of SR and CRs are mentioned in [todo cite].
The definition of SR and CRs are mentioned in \cref{sec:signal-control-regions}. 

The differential cross-section is measured at the particle level in a fiducial phase space. The fiducial phase space in the single-lepton channel is defined by requiring exactly one photon, exactly one electron or muon, and at least four jets with at least one is b-tagged jet, and in the dilepton channel by requiring exactly one photon and two leptons and at least two jets with at least one is b-tagged jet. B-tagged jets at the particle level are obtained using a ghost-matching procedure [todo reference]. For the measurement combining both channels, a union of the single lepton and dilepton fiducial phase space is used. A detailed definition of fiducial phase space is mentioned in \cref{sec:fiducial-phase-space}. The differential cross-section is measured using the profile likelihood unfolding method in which the unfolding problem is transformed to the standard problem of fitting the normalization of distributions. In this approach, a free-floating parameter of interest (POI) is assigned for every bin of the truth distribution. Truth distribution is folded with the response matrix to obtain the reconstructed distribution. The reconstructed distribution is fitted with the profile likelihood method to obtain the POI which fixes the normalization for every bin of the truth distribution, thus we measure the differential cross-section at the particle level. Since the response matrix is quite diagonal, no regularization is applied. Different sources of uncertainties in the measurement are incorporated in the likelihood function as nuisance parameters (NP) with proper constraints. The details of the profile likelihood unfolding method are described in \cref{sec:profile-likelihodd-unfolding}. Sources of uncertainties are mentioned in \cref{sec:sources-of-uncertainties} and the treatment of the uncertainties is detailed in \cref{sec:treatment_of_uncertainties}. The binning for different observables is chosen based on the detector resolution and the statistical uncertainty of that particular bin. The binning is chosen in such a way that the statistical uncertainty in the measured distribution is less than 10\% in each bin except for the last bin. The last bin contains the overflow events. The details on the choice of binning are mentioned in \cref{sec:choice-of-binning}.

Before the fit is performed to real data to measure the cross-section, the fit model is tested with pseudo-data to avoid any post-fit modifications in the model which may introduce bias in the measurement. The pseudo-dataset consists of the sum of the signal and MC templates as well as the fake lepton template. Mainly two tests are performed to test the correct technical implementation of the fit and unfolding procedure as well as the robustness of the method. A closure test is performed by applying the fitting and unfolding procedure to pseudo-data and comparing the results with the known truth distribution. This test helps ensure that the analysis framework is correctly implemented and that the measured quantities are consistent with the expected values. The closure test is detailed in \cref{sec:closure_test}.

On the other hand, a stress test involves intentionally changing the signal in the pseudo-data and measuring the signal with exact same inputs to assess the stability and sensitivity of the analysis. By systematically varying the signal composition in the pseudo-data, the impact on the measured differential cross-sections is evaluated. This test helps quantify whether the unfolding method is biased by the inputs used (truth distribution and response matrix), which were derived from MC. The stress test is detailed in \cref{sec:stress_test}.

Both absolute differential cross-sections and normalized differential cross-sections are measured. The normalized differential cross-sections are measured in such a way that the bin content is normalized to the total cross-section. The normalized differential cross-sections are measured to compare the shape of the distributions between data and MC expectation. The absolute differential cross-sections are measured to compare the absolute cross-sections between data and MC expectation. The measured differential cross-sections are compared with different theoretical predictions. The details of the measurement and the comparison with theoretical predictions are mentioned in \cref{sec:diff-xsec-measurement}.