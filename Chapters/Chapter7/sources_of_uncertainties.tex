\chapter{Sources of uncertainties}
\label{sec:sources-of-uncertainties}
Uncertainties are an inherent part of any measurement, and it is important to properly account for them. There are several sources of uncertainties that need to be considered. These include statistical uncertainties, systematic uncertainties, and model uncertainties. Statistical uncertainties arise from the finite size of our data samples. In collider experiments, events occur randomly with a constant probability. The number of events in a given sample is a random variable that follows a Poisson distribution. The statistical uncertainty is the standard deviation of the Poisson distribution. This is inherent to the data and can only be reduced by increasing the size of the data sample meaning more integrated luminosity.

Systematic uncertainties, on the other hand, arise from imprecision in our experimental setup or analysis techniques. For example, the uncertainty can come from the particle reconstruction and identification, their energy and momentum measurement, luminosity measurement and so on.

Model uncertainties arise from the limitations of our theoretical models used to describe the data. MC event generation is used to generate event from a theoretical model. The uncertainty arises from the choice of the PDF set, the choice of the renormalization and factorization scales, the choice of the parton showering and hadronization models, and so on. Also, the cross-section of every process has an uncertainty associated with it. Different sources of uncertainties are discussed in the following sections. 

\paragraph{Treatment of uncertainties} As mentioned earlier the histogram templates obtained from MC simulated events are used for modelling the signal and background processes. MC templates are produced for different sources of uncertainties. The difference between the nominal template and systematic variation template taken as a measure of the uncertainty. % the question arises how we create the systematic variation templates?
In the statistical model (likelihood) different sources of uncertainties can be assumed as separate auxiliary measurements and can be incorporated in the likelihood function as nuisance parameters (NP), which are constrained to be within their uncertainties. For every source of uncertainty, one NP is added to the fit model. Uncertainties arising from different sources are treated as independent and uncorrelated. This means that the impact of one source of uncertainty does not affect the impact of another source. However, uncertainties within the same source are assumed to be correlated across different signal and control regions. Systematic uncertainties are modeled using Gaussian constraints. On the other hand, statistical uncertainties are modeled using Poisson constraints. The uncertainties associated with the nuisance parameters are estimated from the fit and propagated to the final results.
%discuss about morphing

\paragraph{Smoothing}The smoothing technique is used for some sources of uncertainties to reduce the impact of statistical fluctuations in the template.

\paragraph{Symmetrization}The are some uncertainties for which there are up and down variations w.r.t nominal is available and for some uncertainties only one variation is available. For asymmetric up and down variation, the variation is symmetrized to have a simpler implementation in the fit model. Uncertainties with up and down variations are symmetrized using the Two-Sided symmetrization method in the following way $$ \mathrm{symmetrized \ up/down} = \mathrm{nominal} \pm (\mathrm{up - down})/2 $$ In cases where only one variation is available, the other variation is obtained by mirroring the available one around the nominal. 

\paragraph{Pruning}Uncertainties with negligible impact are pruned from the fit model to avoid using too many NPs in the fit model. The pruning decision is made for shape and normalization components separately. The uncertainty can have a negligible impact on the normalization but a significant impact on the shape. In such cases, the uncertainty is pruned from the normalization component but kept in the shape component. It applies to the other way around as well. For pruning based on the normalization the integral of the template is considered and the relative difference between the nominal is calculated, if the relative difference is less than 0.1\% the uncertainty is pruned. For pruning based on the shape, the relative difference w.r.t nominal is calculated for every bin, if for any bin the difference is more than 0.1\% the uncertainty is kept.


\subsection{Experimental uncertainties}
\label{sec:experimental_uncertainties}
Experimental uncertainties arise from many sources these include uncertainties on the reconstruction and identification of physics objects, uncertainties on the energy and momentum scales and resolutions, uncertainties on the flavour tagging of jets, uncertainties on the integrated luminosity, and uncertainties on the pile-up simulation. These affect both the signal and background simulations. 

\paragraph{Lepton efficiencies}
Electrons are reconstructed from the energy deposits in the calorimeter and the tracks in the inner detector. Muons are reconstructed combining the independent measurement in inner detector and muon spectrometer. These reconstructed leptons are corrected with the \emph{selection efficiency}, these include efficiency related to trigger, particle isolation, identification and reconstruction. These efficiencies are estimated from data using tag-and-probe method using \zee($\mu\mu$ for muons)  or \jpsiee($\mu\mu$ for muons) processes~\cite{ATLAS:2019jvq}\cite{ATLAS:2016lqx}. The multiplicative scale factors are derived from the ratio of efficiencies in data and MC. The scale factors are applied to the MC to correct for the difference in efficiencies. The uncertainty on the estimated scale factors are used to create systematic variation templates.

\paragraph{Photon efficiencies}

\paragraph{$e,\mu,\gamma$ energy and momentum calibration}
The muon momentum is studied using \zmumu and \jpsimumu processes, and correction factors are derived to correct the muon momentum scale and resolution in MC to match with the data~\cite{ATLAS:2016lqx}. 



The measured lepton energy/momentum is calibrated using Monte-Carlo-based techniques. Correction factors, derived from the study of dileptonic decays of the $Z$ boson, are applied to correct possible detector mis-modelling in the calibration. These correction factors, are varied up and down by one standard deviation to study the lepton energy (momentum) scale uncertainty. For electrons, the energy scale and resolution are calculated in conjunction with those of photons and are called $e/\gamma$ scale and resolution.

\subsection*{Photon uncertainties}

The scale factor for the photon ID efficiency is derived from three measurements: the radiative Z boson method using a sample enriched in events with radiative Z boson decays, the electron extrapolation technique using a sample enriched in $Z \to ee$ events where the similarity between electrons and photons in the detector is exploited, and the matrix method using a sample enriched with isolated, high-\pT photons and exploiting that the narrow-strip variables are only weakly correlated to the isolation. The scale factor is computed as the ratio of the efficiency measured in data and that determined in simulation. The sets of scale factors of all measurements are combined into one single set that is applied to simulation to correct for deviations between efficiencies measured in data and found in simulation. The scale factors for photon isolation were measured in Ref.~\cite{ATL-PHYS-PUB-2016-014}. These scale factors are varied up and down by one standard deviation to study their impact on the analysis. The photon energy calibration and the study of its scale and resolution uncertainties are calculated together with the electrons as $e/\gamma$ scale and resolution.


\subsection*{Jet uncertainties}

The jet energy calibration, called jet energy scale (JES), used in this analysis follows the \emph{category reduction scheme} with a total of 30 nuisance parameters~\cite{PERF-2016-04, PERF-2012-01}. The calibration is done in several steps, combining Monte-Carlo simulation and \textit{in situ} measurements, to correct for pile-up, jet flavour composition, single-particle response, and effects of jets not contained within the calorimeter. Its uncertainty is split into several independent categories: modelling and statistical uncertainties on the extrapolation of the jet calibration from the central region, jet flavour composition, high-\pT jet behaviour, \bjet energy scale uncertainties, uncertainties due to pile-up, uncertainties on \textit{in situ} jet energy corrections, etc. In one category, there are usually more than one physical source of the uncertainty. To study the JES uncertainty, each source is varied up and down independently by its corresponding uncertainty. 

The jet energy resolution (JER) is measured using the balance between jets and well measured objects like photons or $Z$ bosons, and it is found to be in agreement between data and MC. There are a total of seven effective nuisance parameters associated to JER in the category reduction scheme, and a single source of uncertainty for the agreement between data and Monte Carlo, all of which are varied by one sigma to study their impact on the analysis.

The systematic uncertainty associated to the jet vertex tagging (JVT) is obtained by varying up and down the JVT cut using the \emph{JetVertexTaggerTool}~\cite{ATLAS-CONF-2014-018}. 


\subsection*{\btag uncertainties}

Jets coming from a \bquarks have their own topological features, and \btag allows to distinguish them from light-flavour jets. With \emph{\btag}, each jet can be assigned to a different working point, and the \btag uncertainties on this jet are derived for this specific working point. They are accounted for by varying the calibration scale factors for \ensuremath{b}-,\ensuremath{c}-, and light-flavour jets up and down by their corresponding systematic uncertainties independently. For each jet category, the uncertainties are decomposed into several uncorrelated components using the eigenvector method. As pseudo-continuous \btag weights are used the corresponding eigenvectors are used, which results in 45 nuisance parameters for \bjets and 20 nuisance parameters for \cjets and light-flavour jets each~\cite{ATL-PHYS-PUB-2017-013}.
%For example, there are 9, 4 and 4 eigenvectors for \bjets, \cjets and light-flavour jets uncertainties, respectively~\cite{ATL-PHYS-PUB-2017-013}.
As the pseudo-continuous b-tagging is only calibrated for b-jets below 400 GeV, c-jets below 250 GeV and light jets below 300 GeV, a normalisation uncertainty was assigned to events containing at least one uncalibrated jet. For each type of jets one uncertainty was defined and for each event containing an uncalibrated jets of that type (b-jet, c-jets or light jet) a 50\% uncertainty was assigned. This leads to three systematic uncertainties.

\subsection*{Missing transverse momentum}

The \met is reconstructed~\cite{ATLAS-CONF-2018-023} from the vector sum of several terms corresponding to different types of reconstructed objects. The estimated uncertainties for electrons, muons, photons and jets are propagated into the uncertainty of \met. Thus, the only new contribution is the systematic uncertainty of the soft terms $E_{\text{x,y}}^{\text{RefSoftJet}}$ and $E_{\text{x,y}}^{\text{CellOut}}$.

The systematic uncertainty of the soft-term scale is estimated by comparing the ratio of Monte-Carlo simulation to data. The average deviation of the ratio from unity is taken as a flat uncertainty on the absolute scale. The systematic uncertainty of the soft-term resolution is estimated by evaluating the level of agreement between data and MC in the $E^{\text{miss}}_{\text{x}}$ and $E^{\text{miss}}_{\text{y}}$ resolution. Both the scale and resolution of the soft term are varied up and down by one standard deviation to study their impact on the analysis. 

\subsection*{Pile-up uncertainties}

The uncertainty on the reweighting procedure used to correct the pile-up profile in MC to match the data, is based on the disagreement between the instantaneous luminosity in data~\cite{DAPR-2013-01} and in simulation. Both the nominal and systematically-shifted pileup-reweighting weights are obtained using the standard \texttt{PileupReweighting} tool~\footnote{More details: \url{https://twiki.cern.ch/twiki/bin/viewauth/AtlasProtected/ExtendedPileupReweighting}.}. 

\subsection*{Luminosity uncertainties}

As quoted in \cref{sec:selection}, the total integrated luminosity has an uncertainty of \intlumiunc.



\subsection{Model uncertainties}
\label{sec:theoretical_uncertainties}


